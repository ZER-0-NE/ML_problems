{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd000b158a6c28b14537e288aace7cd3af6e299df762960738ddd9b5c3154ab3878",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# History and Resources\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/1.png\">\n",
    "</p>\n",
    "\n",
    "- 1943: If you connect very simple binary neurons, you can do logical operations or reasoning. They had an hypothesis that brain has a very large network (actually a fact) of binary neurons which can turn on/off.\n",
    "- 1947: Donald Hebb, a psychologist came up with the proposal that neurons in the brain can change their function by sort of changing the strength of the connection between the neurons. He proposed that if two neurons are active at the same time then the synapse that connects them strengthens and if they're not active then it gets depressed. This is called **Hebbian Learning**.\n",
    "- 1948: Founded a discipline called cybernetics, now called as system theory or stuff like that; out of that came the idea of self-organization - that if you connect lots of very simple elements with a very simple rule to make them compute, then they might be able to self-organize themselves into having an emergent property.\n",
    "- 1957: **Frank Rosenblatt** introduced the idea of Perceptron, which has now become the basis of Supervised Learning. This idea introduced the concept of weights and how they are updated with the corresponding loss. \n",
    "- 1961, 62: Adaline was introduced, much similar to a basic linear classifier. Around that time, neuroscientists discovered some basic properties about the visual cortex i.e., the neurons in the visual cortex basically look at kind of a small area of the visual field and many neurons at different places of individual cortex basically perform similar operation.\n",
    "- 1969: Marvin Minsky publihsed the limits of perceptrons and asked people to work on something else so the field died; people started working on logical reasoning  and stuff.\n",
    "\n",
    "**The main reasons for the field dying off in 1960 are:**\n",
    "\n",
    "- The researchers used neurons that were binary. However, the way to get backpropagation to work is to use activation functions that are continuous. At that time, researchers didn’t have the idea of using continuous neurons and they didn’t think they can train with gradients because binary neurons are not differential.\n",
    "- With continuous neurons, one would have to multiply the activation of a neuron by a weight to get a contribution to the weighted sum. However, before 1980, the multiplication of two numbers, especially floating-point numbers, were extremely slow. This resulted in another incentive to avoid using continuous neurons.\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/2.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/3.png\">\n",
    "</p>\n",
    "\n",
    "- 1970s: People started working on Statistical Pattern Recognition or basically changed the name of what they were working on\n",
    "- 1979: He came up with the idea of Neocognitron getting inspired by Hubel and Weisel and tried to simulate the visual cortex\n",
    "- 1982: Physicists started getting interested in NN; Hopfield came up with the theory for a type of recurrent neural network that he showed could be used as associative memories.\n",
    "- 1983: Introduced Boltzmann machines; answered the cons of previous models like perceptrons, which were single layer models. But these had multiple layers between them, called the hidden layers.\n",
    "- 1985,86: With backprop, a major conceptual change was introduced to switch from binary neurons to continuous neurons because they were differential.\n",
    "- 1989: Yann LeCun introduced CNN's.\n",
    "- 2003: Yoshua bengio came up with neural language model; GPT-3 uses this idea.\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/4.png\">\n",
    "</p>\n",
    "\n",
    "Hot Future things to work on acc. to Yann:\n",
    "- Self-supervised Learning\n",
    "- Transformers in CV\n",
    "- Machine reasoning\n",
    "- Autonomous intelligent machines (AGI he mean)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "<p>\n",
    "    <img src = \"assets/5.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/6.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/7.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/8.png\">\n",
    "</p>\n",
    "\n",
    "### Hierarchical representation of the Visual Cortex\n",
    "\n",
    "Experiments by Fukushima gave us an understanding of how our brain interprets the input to our eyes. In summary, it was discovered that neurons in front of our retina compress the input (known as contrast normalization) and the signal travels from our eyes to our brain. After this, the image gets processed in stages and certain neurons get activated for certain categories. Hence, the visual cortex does pattern recognition in a hierarchical manner.\n",
    "\n",
    "Experiments in which researchers poked electrodes in specific areas of the visual cortex, specifically the V1 area made researchers realize that certain neurons react to motifs that appear in a very small area in a visual field and similarly with neighbouring neurons and neighbouring areas in the visual field. Additionally, neurons that react to the same visual field, react to different types of edges in an organized manner (e.g. vertical or horizontal edges). It is also important to note that there’s also the idea that the visual process is essentially a feed forward process. Hence, somehow fast recognition can be done without some recurrent connections.\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/9.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/10.png\">\n",
    "</p>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}