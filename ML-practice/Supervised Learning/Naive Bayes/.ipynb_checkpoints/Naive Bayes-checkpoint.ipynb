{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Naive Bayes, or called Naive Bayes classifier, is a classifier based on Bayes Theorem with the naive assumption that features are independent of each other.\n",
    "\n",
    "In machine learning, naive Bayes classifiers are simple, probabilistic classifiers that use Bayes’ Theorem. Naive Bayes has strong (naive), independence assumptions between features. In simple terms, a naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a ball may be considered a soccer ball if it is hard, round, and about seven inches in diameter. Even if these features depend on each other or upon the existence of the other features, naive Bayes believes that all of these properties independently contribute to the probability that this ball is a soccer ball. This is why it is known as naive.\n",
    "\n",
    "Naive Bayes models are easy to build. They are also very useful for very large datasets. Although, naive Bayes models are simple, they are known to outperform even the most highly sophisticated classification models. Because they also require a relatively short training time, they make a good alternative for use in classification problems.\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/1.png/\">\n",
    "</p>\n",
    "\n",
    "Here,\n",
    "\n",
    "- P(c|x): posterior probability of class(c,target) given predictor(x,attributes). This represents the probability of c being true, provided x is true.\n",
    "- P(c): is the prior probability ofclass. This is the observed probability of class out of all the observations.\n",
    "- P(x|c): is the likelihood which is the probability of predictor-given class. This represents the probability of x being true, provided x is true.\n",
    "- P(x): is the prior probability ofpredictor. This is the observed probability of predictor out of all the observations.\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/2.png/\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/3.png/\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/4.png/\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/5.png/\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/6.png/\">\n",
    "</p>\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a well-shuffled deck of playing cards. A card is picked from that deck at random. The objective is to find the probability of a King card, given that the card picked is red in color.\n",
    "\n",
    "Here,\n",
    "\n",
    "     P(King | Red Card) = ?\n",
    "\n",
    "We’ll use,\n",
    "\n",
    "     P(King | Red Card) = P(Red Card | King) x P(King) / P(Red Card)\n",
    "\n",
    "So,\n",
    "\n",
    "     P (Red Card | King) = Probability of getting a Red card given that the card chosen is King = 2 Red Kings / 4 Total Kings = ½\n",
    "\n",
    "     P (King) = Probability that the chosen card is a King = 4 Kings / 52 Total Cards = 1 / 13\n",
    "\n",
    "     (Red Card) = Probability that the chosen card is red = 26 Red cards / 52 Total Cards = 1/ 2\n",
    "\n",
    "Hence, finding the posterior probability of randomly choosing a King given a Red card is:\n",
    "\n",
    "     P (King | Red Card) = (1 / 2) x (1 / 13) / (1 / 2) = 1 / 13 or 0.077\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/7.png/\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/8.png/\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/9.png/\">\n",
    "</p>\n",
    "\n",
    "### Why is Naive Bayes so Efficient?\n",
    "\n",
    "There are two reasons that make naive Bayes a very efficient algorithm for classification problems.\n",
    "\n",
    "- Performance: The naive Bayes algorithm gives useful performances despite having correlated variables in the dataset, even though it has a basic assumption of independence among features. The reason for this is that in a given dataset, two attributes may depend on each other, but the dependence may distribute evenly in each of the classes. In this case, the conditional independence assumption of naive Bayes is violated, but it is still the optimal classifier. Further, what eventually affects the classification is the combination of dependencies among all attributes. If we just look at two attributes, there may exist strong dependence between them that affects the classification. When the dependencies among all attributes work together, however, they may cancel each other out and no longer affect the classification. Therefore, we argue that it is the distribution of dependencies among all attributes over classes that affects the classification of naive Bayes, not merely the dependencies themselves.\n",
    "\n",
    "\n",
    "- Speed: The main cause for the fast speed of naive Bayes training is that it converges toward its asymptotic accuracy at a different rate than other methods, like logistic regression, support vector machines, and so on. Naive Bayes parameter estimates converge toward their asymptotic values in order of log(n) examples, where n is number of dimensions. In contrast, logistic regression parameter estimates converge more slowly, requiring order n examples. It is also observed that in several datasets logistic regression outperforms naive Bayes when many training examples are available in abundance, but naive Bayes outperforms logistic regression when training data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code sample 1: MultinomialNB, BernoulliNB, and GaussianNB\n",
    "\n",
    "**The difference between them is the underlying distribution.**\n",
    "\n",
    "**Multi-variate Bernoulli Naive Bayes**:  The binomial model is useful if your feature vectors are binary (i.e., 0s and 1s). One application would be text classification with a bag of words model where the 0s 1s are \"word occurs in the document\" and \"word does not occur in the document\"\n",
    "\n",
    "**Multinomial Naive Bayes**: The multinomial naive Bayes model is typically used for discrete counts. E.g., if we have a text classification problem, we can take the idea of bernoulli trials one step further and instead of \"word occurs in the document\" we have \"count how often word occurs in the document\", you can think of it as \"number of times outcome number x_i is observed over the n trials\"\n",
    "\n",
    "**Gaussian Naive Bayes**: Here, we assume that the features follow a normal distribution. Instead of discrete counts, we have continuous features (e.g., the popular Iris dataset where the features are sepal width, petal width, sepal length, petal length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 4 documents with 6 words(Chinese, Beijing, Shanghai, Macao, Tokyo, Japan) for the training set. \n",
    "# The target class is 0 and 1 respectively\n",
    "\n",
    "X = np.array([\n",
    "    [2,1,0,0,0,0],\n",
    "    [2,0,1,0,0,0],\n",
    "    [1,0,0,1,0,0],\n",
    "    [1,0,0,0,1,1]\n",
    "])\n",
    "y = np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "class MultinomailNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # group by class\n",
    "        seperated = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES:\n",
    "\n",
    "- [Sebastian raschka Naive Bayes and Text Classification\n",
    "](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html)\n",
    "- [All NB implementation](http://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html)\n",
    "- [NB from scratch](https://chrisalbon.com/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/)\n",
    "- [NB scratch numpy](https://geoffruddock.com/naive-bayes-from-scratch-with-numpy/)\n",
    "- [NB scratch python](https://blog.goodaudience.com/building-the-na%C3%AFve-bayes-classifier-from-scratch-in-python-b0717fa022d8)\n",
    "- [NB explained](https://appliedmachinelearning.blog/2017/05/23/understanding-naive-bayes-classifier-from-scratch-python-code/)\n",
    "- [gaussian NB](https://hackernoon.com/implementation-of-gaussian-naive-bayes-in-python-from-scratch-c4ea64e3944d)\n",
    "- [Maths behind NB](https://heartbeat.fritz.ai/understanding-the-mathematics-behind-naive-bayes-ab6ee85f50d0)\n",
    "- [NB maths intel](https://software.intel.com/content/www/us/en/develop/articles/mathematical-concepts-and-principles-of-naive-bayes.html)\n",
    "- [Understanding the mathematics behind Naive Bayes](https://shuzhanfan.github.io/2018/06/understanding-mathematics-behind-naive-bayes/)\n",
    "- [NB scratch tds1](https://towardsdatascience.com/na%C3%AFve-bayes-from-scratch-using-python-only-no-fancy-frameworks-a1904b37222d)\n",
    "- [NB scratchh](https://medium.com/machine-learning-algorithms-from-scratch/naive-bayes-classification-from-scratch-in-python-e3a48bf5f91a)\n",
    "- [NB python tds2 *](https://towardsdatascience.com/how-to-impliment-a-gaussian-naive-bayes-classifier-in-python-from-scratch-11e0b80faf5a)\n",
    "- [Unfolding Naïve Bayes from Scratch !](https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01)\n",
    "- [Naive Bayes Classifier Explained tds3 *](https://towardsdatascience.com/naive-bayes-classifier-explained-54593abe6e18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
