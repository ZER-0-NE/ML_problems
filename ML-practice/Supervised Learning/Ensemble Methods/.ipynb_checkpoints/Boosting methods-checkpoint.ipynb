{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sequential methods the different combined weak models are no longer fitted independently from each others. The idea is to fit models iteratively such that the training of model at a given step depends on the models fitted at the previous steps. “Boosting” is the most famous of these approaches and it produces an ensemble model that is in general less biased than the weak learners that compose it.\n",
    "\n",
    "### Boosting:\n",
    "\n",
    "Boosting methods work in the same spirit as bagging methods: we build a family of models that are aggregated to obtain a strong learner that performs better. However, unlike bagging that mainly aims at reducing variance, boosting is a technique that consists in fitting sequentially multiple weak learners in a very adaptative way: each model in the sequence is fitted giving more importance to observations in the dataset that were badly handled by the previous models in the sequence. Intuitively, each new model focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias (even if we can notice that boosting can also have the effect of reducing variance). Boosting, like bagging, can be used for regression as well as for classification problems.\n",
    "\n",
    "Being mainly focused at reducing bias, the base models that are often considered for boosting are models with low variance but high bias. For example, if we want to use trees as our base models, we will choose most of the time shallow decision trees with only a few depths. Another important reason that motivates the use of low variance but high bias models as weak learners for boosting is that these models are in general less computationally expensive to fit (few degrees of freedom when parametrised). Indeed, as computations to fit the different models can’t be done in parallel (unlike bagging), it could become too expensive to fit sequentially several complex models.\n",
    "\n",
    "Once the weak learners have been chosen, we still need to define how they will be sequentially fitted (what information from previous models do we take into account when fitting current model?) and how they will be aggregated (how do we aggregate the current model to the previous ones?). We will discuss these questions in the two following subsections, describing more especially two important boosting algorithms: adaboost and gradient boosting.\n",
    "\n",
    "**In a nutshell, these two meta-algorithms differ on how they create and aggregate the weak learners during the sequential process. Adaptive boosting updates the weights attached to each of the training dataset observations whereas gradient boosting updates the value of these observations. This main difference comes from the way both methods try to solve the optimisation problem of finding the best model that can be written as a weighted sum of weak learners.**\n",
    "\n",
    "### Adaboost:\n",
    "\n",
    "The name AdaBoost stands for Adaptive Boosting, and it refers to a particular boosting algorithm in which we fit a sequence of “stumps” (decision trees with a single node and two leaves) and weight their contribution to the final vote by how accurate their predictions are. After each iteration, we re-weight the dataset to assign greater importance to data points which were misclassified by the previous weak learner, so that those data points get “special attention” during iteration `t+1`.\n",
    "\n",
    "The following figure illustrates how weights impact the performance of a simple decision stump(tree with depth 1)\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/17.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "There are several different algorithms proposed by researchers. Here I’ll introduce the most popular method called **SAMME, a specific method that deals with multi-classification problems. [(Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009)](https://web.stanford.edu/~hastie/Papers/samme.pdf)**\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/18.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "AdaBoost trains a sequence of models with augmented sample weights, generating ‘confidence’ coefficients Alpha for individual classifiers based on errors. Low errors leads to large Alpha, which means higher importance in the voting.\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/19.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src = \"assets/20.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "    <img src = \"assets/21.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "    <img src = \"assets/22.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "    <img src = \"assets/23.png/\" width = \"600px\" height = \"600px\">\n",
    "    \n",
    "</p>\n",
    "\n",
    "#### AdaBoost.M1 and SAMME are simple generalizations of AdaBoost for more than two classes. In AdaBoost-SAMME the individual trees are required to have an error lower than 1-1/n classes instead of 1/2 of the AdaBoost.M1\n",
    "\n",
    "### Code from scratch 1: Adaboost M1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.2 ],\n",
       "       [0.2 , 0.65],\n",
       "       [0.4 , 0.7 ],\n",
       "       [0.8 , 0.6 ],\n",
       "       [0.8 , 0.3 ],\n",
       "       [0.05, 0.1 ],\n",
       "       [0.08, 0.4 ],\n",
       "       [0.12, 0.66],\n",
       "       [0.33, 0.77],\n",
       "       [0.55, 0.65],\n",
       "       [0.66, 0.68],\n",
       "       [0.77, 0.55],\n",
       "       [0.88, 0.44],\n",
       "       [0.2 , 0.1 ],\n",
       "       [0.3 , 0.3 ],\n",
       "       [0.4 , 0.4 ],\n",
       "       [0.5 , 0.3 ],\n",
       "       [0.6 , 0.15],\n",
       "       [0.25, 0.15],\n",
       "       [0.3 , 0.5 ],\n",
       "       [0.5 , 0.55],\n",
       "       [0.7 , 0.2 ],\n",
       "       [0.6 , 0.4 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset and utitlity plotting function\n",
    "\n",
    "x1 = np.array([.1,.2,.4,.8, .8, .05,.08,.12,.33,.55,.66,.77,.88,.2,.3,.4,.5,.6,.25,.3,.5,.7,.6])\n",
    "x2 = np.array([.2,.65,.7,.6, .3,.1,.4,.66,.77,.65,.68,.55,.44,.1,.3,.4,.3,.15,.15,.5,.55,.2,.4])\n",
    "y = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
    "X = np.vstack((x1,x2)).T\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(classifier, X, y, N=10, scatter_weights = np.ones(len(y)), ax = None):\n",
    "    '''\n",
    "    Utility function to plot decision boundary and scatter plot of data\n",
    "    '''\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
    "    \n",
    "     #Check what methods are available\n",
    "    if hasattr(classifier, \"decision_function\"):\n",
    "        zz = np.array( [classifier.decision_function(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    elif hasattr(classifier, \"predict_proba\"):\n",
    "        zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    else :\n",
    "        zz = np.array( [classifier(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    \n",
    "    # reshape result and plot\n",
    "    Z = zz.reshape(xx.shape)\n",
    "    cm_bright =  ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    # Get current axis and plot\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, 2, cmap='RdBu', alpha=.5)\n",
    "    ax.contour(xx, yy, Z,  2, cmap='RdBu')\n",
    "    ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 40)\n",
    "    ax.set_xlabel('$X_1$')\n",
    "    ax.set_ylabel('$X_2$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sklearn adaboost\n",
    "boost = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes = 2), algorithm = 'SAMME',n_estimators=10, learning_rate=1.0)\n",
    "boost.fit(X, y)\n",
    "plot_decision_boundary(boost, X,y, N = 50)#, weights)\n",
    "plt.show()\n",
    "\n",
    "boost.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adaboost_scratch(X, y, M=10,learning_rate = 1):\n",
    "    '''Initialization of utility variables'''\n",
    "    N = len(y)\n",
    "    estimator_list, y_predict_list, estimator_error_list, estimator_weight_list, sample_weight_list = [], [],[],[],[]\n",
    "    \n",
    "    # Initialize the sample weights\n",
    "    sample_weight = np.ones(N) /N\n",
    "    sample_weight_list.append(sample_weight.copy())\n",
    "    \n",
    "    for m in range(M):\n",
    "        \n",
    "        #Fit a classifier\n",
    "        estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes=2)\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        #Misclassifications\n",
    "        incorrect = (y_predict != y)\n",
    "        \n",
    "        #Estimator error\n",
    "        estimator_error = np.mean(np.average(incorrect, weights = sample_weight, axis = 0))\n",
    "        \n",
    "        #Boost estimator weights\n",
    "        estimator_weight = learning_rate * np.log((1. - estimator_error)/estimator_error)\n",
    "        \n",
    "        # Boost sample weights\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect * ((sample_weight>0) | (estimator_weight<0)))\n",
    "        \n",
    "        #Save iteration values\n",
    "        estimator_list.append(estimator)\n",
    "        y_predict_list.append(y_predict.copy())\n",
    "        estimator_error_list.append(estimator_error.copy())\n",
    "        estimator_weight_list.append(estimator_weight.copy())\n",
    "        sample_weight_list.append(sample_weight.copy())\n",
    "    \n",
    "    #Convert to np array for convenience   \n",
    "    estimator_list = np.asarray(estimator_list)\n",
    "    y_predict_list = np.asarray(y_predict_list)\n",
    "    estimator_error_list = np.asarray(estimator_error_list)\n",
    "    estimator_weight_list = np.asarray(estimator_weight_list)\n",
    "    sample_weight_list = np.asarray(sample_weight_list)\n",
    "    \n",
    "    #Predictions\n",
    "    preds = (np.array([np.sign((y_predict_list[:,point] * estimator_weight_list).sum()) for point in range(N)]))\n",
    "    print('Accuracy = ', (preds == y).sum() / N) \n",
    "    \n",
    "    return estimator_list, estimator_weight_list, sample_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "estimator_list, estimator_weight_list, sample_weight_list  = Adaboost_scratch(X,y, M=10, learning_rate = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERNCES: \n",
    "\n",
    "- [Gradient Boosting in Python from Scratch*](https://towardsdatascience.com/gradient-boosting-in-python-from-scratch-4a3d9077367#:~:text=Gradient%20boosting%20is%20an%20ensemble,what%20it's%20given%20as%20inputs.)\n",
    "- [Gradient Boosting from scratch*](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n",
    "- [Gradient Boosting (GBM) from Scratch - [Tutorial]](https://steemit.com/machine-learning/@cristi/gradient-boosting-gbm-from-scratch-tutorial)\n",
    "- [How to explain gradient boosting](https://explained.ai/gradient-boosting/)\n",
    "- [Demystifying Maths of Gradient Boosting](https://towardsdatascience.com/demystifying-maths-of-gradient-boosting-bd5715e82b7c#:~:text=The%20idea%20is%20simple%2D%20form,suitable%20number%20of%20base%20learners.)\n",
    "- [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "- [The Gradient Boosters I: The Good Old Gradient Boosting](https://deep-and-shallow.com/2020/02/02/the-gradient-boosters-i-the-math-heavy-primer-to-gradient-boosting-algorithm/)\n",
    "- [Gradient Boosting Explained](https://www.gormanalysis.com/blog/gradient-boosting-explained/)\n",
    "- [Math behind GBM and XGBoost](https://medium.com/analytics-vidhya/math-behind-gbm-and-xgboost-d00e8536b7de)\n",
    "- [Gradient Boosting Decision Tree Algorithm Explained*](https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4)\n",
    "- []()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
