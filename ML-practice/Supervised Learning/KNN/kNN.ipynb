{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN introduction:\n",
    "\n",
    "K Nearest Neighbor (KNN from now on) is one of those algorithms that are very simple to understand but works incredibly well in practice. Also it is surprisingly versatile and its applications range from vision to proteins to computational geometry to graphs and so on .\n",
    "\n",
    "**KNN is an non parametric lazy learning algorithm.** That is a pretty concise statement. When you say a technique is **non parametric** , it means that it does not make any assumptions on the underlying data distribution. This is pretty useful , as in the real world , most of the practical data does not obey the typical theoretical assumptions made (eg gaussian mixtures, linearly separable etc) . Non parametric algorithms like KNN come to the rescue here.\n",
    "\n",
    "It is also a **lazy algorithm**. What this means is that it does not use the training data points to do any generalization. In other words, there is no explicit training phase or it is very minimal. This means the training phase is pretty fast . Lack of generalization means that KNN keeps all the training data. More exactly, all the training data is needed during the testing phase. (Well this is an exaggeration, but not far from truth). This is in contrast to other techniques like SVM where you can discard all non support vectors without any problem.  Most of the lazy algorithms – especially KNN – makes decision based on the entire training data set (in the best case a subset of them).\n",
    "\n",
    "The dichotomy is pretty obvious here – **There is a non existent or minimal training phase but a costly testing phase.** The cost is in terms of both time and memory. More time might be needed as in the worst case, all data points might take point in decision. More memory is needed as we need to store all training data.\n",
    "\n",
    "### Assumptions in KNN\n",
    "Before using KNN, let us revisit some of the assumptions in KNN.\n",
    "\n",
    "KNN assumes that the data is in a feature space. More exactly, the data points are in a metric space. The data can be scalars or possibly even multidimensional vectors. Since the points are in feature space, they have a notion of distance – This need not necessarily be Euclidean distance although it is the one commonly used.\n",
    "\n",
    "Each of the training data consists of a set of vectors and class label associated with each vector. In the simplest case , it will be either + or – (for positive or negative classes). But KNN , can work equally well with arbitrary number of classes.\n",
    "\n",
    "We are also given a single number \"k\" . This number decides how many neighbors (where neighbors is defined based on the distance metric) influence the classification. This is usually a odd number if the number of classes is 2. If k=1 , then the algorithm is simply called the nearest neighbor algorithm.\n",
    "\n",
    "\n",
    "### KNN for Density Estimation\n",
    "\n",
    "Although classification remains the primary application of KNN, we can use it to do density estimation also. Since KNN is non parametric, it can do estimation for arbitrary distributions. The idea is very similar to use of [Parzen window](http://en.wikipedia.org/wiki/Parzen_window) . Instead of using hypercube and kernel functions, here we do the estimation as follows – For estimating the density at a point x, place a hypercube centered at x and keep increasing its size till k neighbors are captured. Now estimate the density using the formula,\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/1.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Where n is the total number of V is the volume of the hypercube. Notice that the numerator is essentially a constant and the density is influenced by the volume. The intuition is this : Lets say density at x is very high. Now, we can find k points near x very quickly . These points are also very close to x (by definition of high density). This means the volume of hypercube is small and the resultant density is high. Lets say the density around x is very low. Then the volume of the hypercube needed to encompass k nearest neighbors is large and consequently, the ratio is low.\n",
    "\n",
    "The volume performs a job similar to the bandwidth parameter in kernel density estimation. In fact , KNN is one of common methods to estimate the bandwidth (eg adaptive mean shift) .\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/2.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/3.png\">\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/4.png\" width = 300px height = 300px>\n",
    "        <img src = \"assets/5.png\" width = 300px height = 300px>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code sample 1: kNN using sklearn + scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define column names\n",
    "names = [\n",
    "  'sepal_length',\n",
    "  'sepal_width',\n",
    "  'petal_length',\n",
    "  'petal_width',\n",
    "  'class',\n",
    "]\n",
    "\n",
    "# load training data\n",
    "df = pd.read_csv('iris.data', header=None, names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create design matrix X and target vector y\n",
    "X = np.array(df.iloc[:, 0:4])  # end index is exclusive\n",
    "y = np.array(df['class'])    # another way of indexing a pandas df\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# instantiate learning model (k = 3)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning with Cross Validation\n",
    "\n",
    "Obviously, the best K is the one that corresponds to the lowest test error rate, so let’s suppose we carry out repeated measurements of the test error for different values of K. Inadvertently, what we are doing is using the test set as a training set! This means that we are underestimating the true error rate since our model has been forced to fit the test set in the best possible manner. Our model is then incapable of generalizing to newer observations, a process known as overfitting. Hence, touching the test set is out of the question and must only be done at the very end of our pipeline.\n",
    "\n",
    "`Using the test set for hyperparameter tuning can lead to overfitting.`\n",
    "\n",
    "An alternative and smarter approach involves estimating the test error rate by holding out a subset of the training set from the fitting process. This subset, called the `validation set`, can be used to select the appropriate level of flexibility of our algorithm! There are different validation approaches that are used in practice, and we will be exploring one of the more popular ones called k-fold cross validation.\n",
    "\n",
    "<p>\n",
    "        <img src = \"assets/6.png\" width = 700px height = 700px>\n",
    "</p>\n",
    "\n",
    "As seen in the image, k-fold cross validation (the k is totally unrelated to K) involves randomly dividing the training set into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k−1 folds. The misclassification rate is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. This process results in k estimates of the test error which are then averaged out.\n",
    "\n",
    "`Cross-validation can be used to estimate the test error associated with a learning method in order to evaluate its performance, or to select the appropriate level of flexibility.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# creating odd list of k for kNN\n",
    "neighbors = list(range(1, 50, 2))\n",
    "\n",
    "# holding cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10 fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    score = cross_val_score(knn, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "    cv_scores.append(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# changing to misclassification error\n",
    "mse = [1-x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[mse.index(min(mse))]\n",
    "print(f\"The optimal number of neighbors is {optimal_k}\")\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, mse)\n",
    "plt.xlabel(\"Number of neighbors K\")\n",
    "plt.ylabel(\"Misclassification error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross validation tells us that `K=1` results in the lowest validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning algorithm usually consists of 2 main blocks:\n",
    "\n",
    "- a **training block** that takes as input the training data X and the corresponding target y and outputs a learned model h.\n",
    "\n",
    "- a **predict block** that takes as input new and unseen observations and uses the function h to output their corresponding responses.\n",
    "\n",
    "In the case of KNN, which as discussed earlier, is a lazy algorithm, the training block reduces to just memorizing the training data. Let’s go ahead a write a python method that does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    # do nothing\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write the predict method which must do the following: it needs to compute the euclidean distance between the “new” observation and all the data points in the training set. It must then select the K nearest ones and perform a majority vote. It then assigns the corresponding label to the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, x_test, k):\n",
    "    distances = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        distances.append([np.sqrt(np.sum(np.square(x_test - X_train[i,:]))), i])\n",
    "    \n",
    "    distances = sorted(distances)\n",
    "    \n",
    "    # make a list of the k neighbors targets\n",
    "    for i in range(k):\n",
    "        index = distances[i][1]\n",
    "        targets.append(y_train[index])\n",
    "    \n",
    "    # return most common target\n",
    "    return Counter(targets).most_common(1)[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we create an array of distances which we sort by increasing order. That way, we can grab the K nearest neighbors (first K distances), get their associated labels which we store in the targets array, and finally perform a majority vote using a Counter.\n",
    "\n",
    "Putting it all together, we can define the function k_nearest_neighbor, which loops over every test example and makes a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNearestNeighbors(X_train, y_train, X_test, k):\n",
    "    # check if k larger than n\n",
    "    assert k <= len(X_train), \"[!] k can't be larger than number of samples.\"\n",
    "    \n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # loop over all observations\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "    \n",
    "    return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our classifier is 0.98\n"
     ]
    }
   ],
   "source": [
    "# making our predictions\n",
    "predictions = kNearestNeighbors(X_train, y_train, X_test, 7)\n",
    "\n",
    "# evaluating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"The accuracy of our classifier is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of KNN\n",
    "\n",
    "**Pros:** As you can already tell from the previous section, one of the most attractive features of the K-nearest neighbor algorithm is that is simple to understand and easy to implement. With zero to little training time, it can be a useful tool for off-the-bat analysis of some data set you are planning to run more complex algorithms on. Furthermore, KNN works just as easily with multiclass data sets whereas other algorithms are hardcoded for the binary setting. Finally, as we mentioned earlier, the non-parametric nature of KNN gives it an edge in certain settings where the data may be highly “unusual”.\n",
    "\n",
    "**Cons:** One of the obvious drawbacks of the KNN algorithm is the computationally expensive testing phase which is impractical in industry settings. Note the rigid dichotomy between KNN and the more sophisticated Neural Network which has a lengthy training phase albeit a very fast testing phase. Furthermore, KNN can suffer from skewed class distributions. For example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common). Finally, the accuracy of KNN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES:\n",
    "\n",
    "- [knn scratch](https://kraj3.com.np/blog/2019/06/implementation-of-knn-from-scratch-in-python/)\n",
    "- [knn from scratch](https://dataaspirant.com/2016/12/27/k-nearest-neighbor-algorithm-implementaion-python-scratch/)\n",
    "- [Mail spam or not - kNN](https://anujkatiyal.com/blog/2017/10/01/ml-knn/#.XrZCVnUzZuQ)\n",
    "- [kNN in depth analysis](https://tomaszgolan.github.io/introduction_to_machine_learning/markdown/introduction_to_machine_learning_01_knn/introduction_to_machine_learning_01_knn/)\n",
    "- [kNN numpy NYC](https://nycdatascience.com/blog/student-works/machine-learning/knn-classifier-from-scratch-numpy-only/)\n",
    "- [Maths kNN](http://www.datascribble.com/blog/machine-learning/understanding-math-behind-knn-codes-python/)\n",
    "- [A Complete Guide to K-Nearest-Neighbors with Applications in Python and R](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/)\n",
    "- [kNN tds1](https://towardsdatascience.com/knn-k-nearest-neighbors-1-a4707b24bd1d)\n",
    "- [KNN ALGORITHM AND IMPLEMENTATION FROM SCRATCH]()\n",
    "- [Knn](https://medium.com/datadriveninvestor/knn-algorithm-and-implementation-from-scratch-b9f9b739c28f)\n",
    "- [kNN from scratch](https://towardsdatascience.com/lets-make-a-knn-classifier-from-scratch-e73c43da346d)\n",
    "- [kNN numpy scratch](https://towardsdatascience.com/k-nearest-neighbors-classification-from-scratch-with-numpy-cb222ecfeac1)\n",
    "- [ML basics with kNN](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n",
    "- [A Detailed Introduction to K-Nearest Neighbor (KNN) Algorithm](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/)\n",
    "- []()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
